{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221074ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6522bb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2880, 1800),\n",
       " (2560, 1600),\n",
       " (2048, 1280),\n",
       " (1920, 1200),\n",
       " (1680, 1050),\n",
       " (1440, 900),\n",
       " (1280, 800),\n",
       " (1024, 768),\n",
       " (1024, 640),\n",
       " (840, 525),\n",
       " (800, 600),\n",
       " (720, 450),\n",
       " (640, 480)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pygame.init()\n",
    "pygame.display.list_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8ed187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished in 30 steps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "for t in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # random action\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished in {} steps\".format(t))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3891c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd007b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a2d602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "535d4529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "12\n",
      "1\n",
      "4\n",
      "6\n",
      "6\n",
      "1\n",
      "4\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#sample from the state space\n",
    "for _ in range(10):\n",
    "    print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025be415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "128afbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#sample from the state space\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21c7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize env\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test action 1\n",
    "env.reset()\n",
    "action = 1\n",
    "for _ in range(5):\n",
    "    env.render()\n",
    "    state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded trajectory\n",
    "env.reset()\n",
    "optimal_actions = [1,1,2,1,2,2]\n",
    "for t in range(len(optimal_actions)):\n",
    "    env.render()\n",
    "    action = optimal_actions[t]\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('s = {}, r = {}, d = {}'.format(state, reward, done))\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb86b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851989c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'solutions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import utils\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'solutions'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import utils\n",
    "import solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70aa144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67412c4e",
   "metadata": {},
   "source": [
    "num_steps = env._max_episode_steps\n",
    "num_states = env.oservation_space.n\n",
    "num_actions = env.action_space.n\n",
    "env_model = env.P\n",
    "gamma = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy iteration\n",
    "# loop thorough all of our actions through all possible next states...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fbce61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with a random policy\n",
    "def random_policy(num_states, num_actions):\n",
    "    return np.ones((num_states,num_actions)) / num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7198d32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mutils\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "utilsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning\n",
    "# no access to environmental model (probabilities)\n",
    "# must use noise or exlporation and iterative calculation of probability\n",
    "num_training_episodes = 5000\n",
    "num_eval_episodes = 50\n",
    "num_steps = env._max_episode_steps\n",
    "num_states = env.observation_sapce.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "gamma = 1\n",
    "alpha = .1\n",
    "max_epsilon = 1\n",
    "min_epsilon = .01\n",
    "epsilon_decay = .005\n",
    "eval_itrs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) for all s _E_ S and a _E_ A\n",
    "# For each Episode:\n",
    "#     reset evnireonment, roll out trajectory?\n",
    "#     select action a_t for state s_t (policy derived from (s,a)).\n",
    "#     take action a_t, observe reward r_t+1 and next state s_t+1.\n",
    "#     Update Q(s_t,a_t) := Q(s_t,a_t) + alpha[r_t+1 + gamma* max(a)Q(s_t+1, a)- Q(s_t,a_t)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44611e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment\n",
    "# Definitions \n",
    "# 8x8 environment, totalling 64 states, \n",
    "# all states have reward of 0, except (7,7)\n",
    "# the bottom right. \n",
    "# \n",
    "# policy iteration v policy evaluation\n",
    "# \n",
    "# policy is state x action matrix offf probabilities?\n",
    "# training loop\n",
    "def evaluate_policy(policy, num_episodes=10, render=False):\n",
    "    returns = list()\n",
    "    for e in range(num_episodes):\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        \n",
    "        t = 0 \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                \n",
    "            # takes best action\n",
    "            action = np.argmax(policy[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                returns.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    return returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1209c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_policy():\n",
    "    # initiate random policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ea513f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation\n",
    "# def policy_evaluation(s, t):\n",
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "        # Number of evaluation iterations\n",
    "        evaluation_iterations = 1\n",
    "        # Initialize a value function for each state as zero\n",
    "        V = np.zeros(environment.observation_space.n)\n",
    "        # Repeat until change in value is below the threshold\n",
    "        for i in range(int(max_iterations)):\n",
    "                # Initialize a change of value function as zero\n",
    "                delta = 0\n",
    "                # Iterate though each state\n",
    "                for state in range(environment.observation_space.n):\n",
    "                       # Initial a new value of current state\n",
    "                       v = 0\n",
    "                       # Try all possible actions which can be taken from this state\n",
    "                       for action, action_probability in enumerate(policy[state]):\n",
    "                             # Check how good next state will be\n",
    "                             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                                  # Calculate the expected value\n",
    "                                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "                       \n",
    "                       # Calculate the absolute change of value function\n",
    "                       delta = max(delta, np.abs(V[state] - v))\n",
    "                       # Update value function\n",
    "                       V[state] = v\n",
    "                evaluation_iterations += 1\n",
    "                \n",
    "                # Terminate if value change is insignificant\n",
    "                if delta < theta:\n",
    "                        print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "                        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46e95759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "        action_values = np.zeros(environment.action_space.n)\n",
    "        for action in range(environment.action_space.n):\n",
    "                for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                        action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f75770bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "        # Start with a random policy\n",
    "        #num states x num actions / num actions\n",
    "        policy = np.ones([environment.observation_space.n, environment.action_space.n]) / environment.action_space.n\n",
    "        \n",
    "        \n",
    "        # Initialize counter of evaluated policies\n",
    "        evaluated_policies = 1\n",
    "        # Repeat until convergence or critical number of iterations reached\n",
    "        for i in range(int(max_iterations)):\n",
    "                stable_policy = True\n",
    "                # Evaluate current policy\n",
    "                V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "                # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "                for state in range(environment.observation_space.n):\n",
    "                        # Choose the best action in a current state under current policy\n",
    "                        current_action = np.argmax(policy[state])\n",
    "                        # Look one step ahead and evaluate if current action is optimal\n",
    "                        # We will try every possible action in a current state\n",
    "                        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "                        # Select a better action\n",
    "                        best_action = np.argmax(action_value)\n",
    "                        # If action didn't change\n",
    "                        if current_action != best_action:\n",
    "                                stable_policy = True\n",
    "                                # Greedy policy update\n",
    "                                policy[state] = np.eye(environment.action_space.n)[best_action]\n",
    "                evaluated_policies += 1\n",
    "                # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "                if stable_policy:\n",
    "                        print(f'Evaluated {evaluated_policies} policies.')\n",
    "                        return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59f136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "841ef779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n",
      "Evaluated 2 policies.\n",
      "Policy Iteration :: number of wins over 10000 episodes = 7259\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.7259 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "        wins = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(n_episodes):\n",
    "                terminated = False\n",
    "                state = environment.reset()\n",
    "                while not terminated:\n",
    "                        # Select best action to perform in a current state\n",
    "                        action = np.argmax(policy[state])\n",
    "                        # Perform an action an observe how environment acted in response\n",
    "                        next_state, reward, terminated, info = environment.step(action)\n",
    "                        # Summarize total reward\n",
    "                        total_reward += reward\n",
    "                        # Update current state\n",
    "                        state = next_state\n",
    "                        # Calculate number of wins over episodes\n",
    "                        if terminated and reward == 1.0:\n",
    "                                wins += 1\n",
    "        average_reward = total_reward / n_episodes\n",
    "        return wins, total_reward, average_reward\n",
    "\n",
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "# Functions to find best policy\n",
    "solvers = [\n",
    "    ('Policy Iteration', policy_iteration),\n",
    "]\n",
    "for iteration_name, iteration_func in solvers:\n",
    "        # Load a Frozen Lake environment\n",
    "        environment = gym.make('FrozenLake-v1')\n",
    "        # Search for an optimal policy using policy iteration\n",
    "        policy, V = iteration_func(environment.env)\n",
    "        # Apply best policy to the real environment\n",
    "        wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "        print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "        print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f9b1eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.class_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
